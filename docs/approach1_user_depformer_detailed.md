# アプローチ1: 専用Depformer分岐によるユーザー発話完全予測

## 概要

このアプローチでは、既存のシステム発話用Depformerと並行して、**ユーザー発話予測専用のDepformer分岐**を追加します。これにより、モデルは未来のユーザー音声コードを明示的に予測できるようになります。

## 1. アーキテクチャ設計

### 1.1 現在のアーキテクチャ

```
入力コード [B, K_total, T]
    ↓
[Main Transformer] → transformer_out [B, T, dim]
    ├─ [Text Linear] → text_logits [B, 1, T, text_card]
    └─ [Depformer (System)] → system_audio_logits [B, dep_q, T, card]
```

**現在のコードブック構成**:
```python
# K_total = 1 + dep_q + n_q_user
# Index 0: Text (inner monologue)
# Index 1 to dep_q: System audio codebooks (generated by Depformer)
# Index dep_q+1 onwards: User audio codebooks (input, not predicted)
```

### 1.2 提案するアーキテクチャ

```
入力コード [B, K_total, T]
    ↓
[Main Transformer] → transformer_out [B, T, dim]
    ├─ [Text Linear] → text_logits [B, 1, T, text_card]
    ├─ [Depformer (System)] → system_audio_logits [B, dep_q, T, card]
    └─ [Depformer (User)] → user_audio_logits [B, n_q_user, T, card]  ← NEW!
```

**拡張後のコードブック構成**:
```python
# 訓練時: 全コードブックが既知
# 推論時:
#   - Text: 生成
#   - System audio: 生成 (既存のDepformer)
#   - User audio: 生成 (新しいDepformer) または 実際の入力を使用
```

## 2. 実装の詳細

### 2.1 モデルアーキテクチャの拡張

#### 2.1.1 LMModelクラスの変更

参照: `moshi/models/lm.py:75-239`

```python
class LMModel(StreamingContainer):
    def __init__(
        self,
        # 既存のパラメータ
        delays: tp.List[int] = [0],
        n_q: int = 8,                    # システムオーディオのコードブック数
        dep_q: int = 8,                  # Depformerで生成するコードブック数
        card: int = 1024,
        text_card: int = 32000,
        dim: int = 128,

        # 新規パラメータ: ユーザー予測用
        predict_user_audio: bool = False,           # ユーザー音声予測を有効化
        user_n_q: int = 8,                          # ユーザーコードブック数
        user_dep_q: int = 8,                        # ユーザーDepformerで生成する数
        user_prediction_lookahead: int = 1,         # 何ステップ先まで予測するか

        # ユーザーDepformer専用パラメータ
        user_depformer_dim: int = 256,
        user_depformer_dim_feedforward: int | None = None,
        user_depformer_multi_linear: bool = False,
        user_depformer_weights_per_step: bool = False,
        user_depformer_pos_emb: str = "sin",
        user_depformer_norm: str | None = None,

        # その他既存パラメータ
        **kwargs,
    ):
        super().__init__()
        self.n_q = n_q
        self.dep_q = dep_q
        self.card = card
        self.text_card = text_card
        self.dim = dim

        # ユーザー予測関連
        self.predict_user_audio = predict_user_audio
        self.user_n_q = user_n_q
        self.user_dep_q = user_dep_q
        self.user_prediction_lookahead = user_prediction_lookahead

        # 既存の埋め込み層とトランスフォーマー
        # ... (省略)

        # テキストとシステムオーディオ用の既存コンポーネント
        self.transformer = StreamingTransformer(...)
        self.text_linear = nn.Linear(dim, text_card_out, bias=bias_proj)

        # システムDepformer（既存）
        if dep_q > 0:
            self._build_system_depformer(
                dep_q, depformer_dim, card,
                depformer_multi_linear, **kwargs
            )

        # ユーザーDepformer（新規）
        if predict_user_audio and user_dep_q > 0:
            self._build_user_depformer(
                user_dep_q, user_depformer_dim, card,
                user_depformer_multi_linear,
                user_depformer_dim_feedforward,
                user_depformer_weights_per_step,
                user_depformer_pos_emb,
                user_depformer_norm,
                **kwargs
            )

    def _build_user_depformer(
        self,
        user_dep_q: int,
        user_depformer_dim: int,
        card: int,
        multi_linear: bool,
        dim_feedforward: int | None,
        weights_per_step: bool,
        pos_emb: str,
        norm: str | None,
        **kwargs
    ):
        """ユーザー予測用Depformerの構築"""

        # メイントランスフォーマー出力からユーザーDepformerへの投影
        if multi_linear:
            self.user_depformer_in = nn.ModuleList([
                nn.Linear(self.dim, user_depformer_dim, bias=False)
                for _ in range(user_dep_q)
            ])
        else:
            self.user_depformer_in = nn.ModuleList([
                nn.Linear(self.dim, user_depformer_dim, bias=False)
            ])

        # ユーザー音声コードの埋め込み
        # 既存のユーザーコードを条件として使うため
        EmbeddingFactory = partial(
            ScaledEmbedding,
            norm=self.norm_emb,
            device=self.device,
            dtype=self.dtype,
            zero_idx=self.zero_token_id,
        )

        self.user_depformer_emb = nn.ModuleList([
            EmbeddingFactory(self.card + 1, user_depformer_dim)
            for _ in range(user_dep_q - 1)
        ])

        # テキスト埋め込みも使用（システムの発話計画を反映）
        self.user_depformer_text_emb = EmbeddingFactory(
            self.text_card + 1,
            user_depformer_dim,
        )

        # 正規化層
        if norm is None:
            self.user_depformer_norms = nn.ModuleList([
                nn.Identity() for _ in range(user_dep_q)
            ])
        else:
            self.user_depformer_norms = nn.ModuleList([
                create_norm_fn(norm, user_depformer_dim)
                for _ in range(user_dep_q)
            ])

        # Depformerトランスフォーマー
        if dim_feedforward is None:
            dim_feedforward = int(self.hidden_scale * user_depformer_dim)

        kwargs_user_dep = {
            'positional_embedding': pos_emb,
            'context': None,
            'cross_attention': False,
            'causal': self.causal,
        }

        if weights_per_step:
            kwargs_user_dep['weights_per_step'] = user_dep_q

        self.user_depformer = StreamingTransformer(
            d_model=user_depformer_dim,
            dim_feedforward=dim_feedforward,
            norm=self.norm,
            quantize=self.quantize,
            device=self.device,
            dtype=self.dtype,
            **kwargs_user_dep,
        )

        # Depformerは1タイムステップ内で完結するストリーミング
        self.user_depformer.set_streaming_detached(True)

        # 出力線形層（各ユーザーコードブック用）
        self.user_linears = nn.ModuleList([
            nn.Linear(user_depformer_dim, self.card, bias=self.bias_proj)
            for _ in range(user_dep_q)
        ])

    @property
    def num_user_codebooks(self) -> int:
        """ユーザーのコードブック総数"""
        return self.user_n_q if self.predict_user_audio else 0

    @property
    def user_audio_offset(self) -> int:
        """ユーザーオーディオコードブックの開始インデックス"""
        return 1 + self.dep_q  # text + system_audio の後
```

#### 2.1.2 ユーザー予測のForwardメソッド

```python
class LMModel(StreamingContainer):
    # ... (上記の__init__の続き)

    def forward_user_depformer(
        self,
        user_depformer_cb_index: int,
        sequence: torch.Tensor,
        transformer_out: torch.Tensor,
        predicted_text_token: torch.Tensor | None = None,
    ) -> torch.Tensor:
        """
        ユーザーDepformerで1つのコードブックを予測

        Args:
            user_depformer_cb_index: 予測するユーザーコードブックのインデックス (0 to user_dep_q-1)
            sequence: 既に予測されたユーザーコード [B, K, S]
            transformer_out: メイントランスフォーマーの出力 [B, S, dim]
            predicted_text_token: 予測されたテキストトークン [B, S] (オプション)

        Returns:
            logits: [B, 1, S, card]
        """
        assert hasattr(self, 'user_depformer'), "User depformer not initialized"

        B, K, S = sequence.shape
        assert K == 1, f"Codebooks should be passed 1 by 1, got {K}"
        assert S == 1, f"Steps should be passed 1 by 1, got {S}"
        assert transformer_out.shape[1] == 1, "Transformer out should be for a single step"

        # メイントランスフォーマー出力を投影
        if self.user_depformer_multi_linear:
            depformer_input = self.user_depformer_in[user_depformer_cb_index](transformer_out)
        else:
            depformer_input = self.user_depformer_in[0](transformer_out)

        # 前のトークンの埋め込みを追加
        if user_depformer_cb_index == 0:
            # 最初のコードブックは、テキストトークンを条件とする
            if predicted_text_token is not None:
                last_token_input = self.user_depformer_text_emb(predicted_text_token)
            else:
                # テキストトークンがない場合は、sequenceの最初を使用
                last_token_input = self.user_depformer_text_emb(sequence[:, 0])
        else:
            # 前のユーザーコードブックの出力を条件とする
            last_token_input = self.user_depformer_emb[user_depformer_cb_index - 1](
                sequence[:, 0]
            )

        depformer_input = depformer_input + last_token_input

        # Depformerを適用
        dep_output = self.user_depformer(depformer_input)

        # 正規化と線形層
        logits = self.user_linears[user_depformer_cb_index](
            self.user_depformer_norms[user_depformer_cb_index](dep_output)
        )

        logits = logits[:, None]  # [B, 1, S, card]

        return logits

    def forward_user_depformer_training(
        self,
        sequence: torch.Tensor,
        transformer_out: torch.Tensor,
    ) -> torch.Tensor:
        """
        訓練時のユーザーDepformer（全コードブックを一度に処理）

        Args:
            sequence: [B, K_total, T] - 全コードブック
            transformer_out: [B, T, dim]

        Returns:
            user_logits: [B, user_dep_q, T, card]
        """
        if not self.predict_user_audio:
            return None

        B, K, T = sequence.shape
        user_offset = self.user_audio_offset

        # ユーザーコードブックを抽出
        user_codes = sequence[:, user_offset:user_offset + self.user_dep_q, :]

        all_logits = []

        for cb_idx in range(self.user_dep_q):
            # メイントランスフォーマー出力を投影
            if self.user_depformer_multi_linear:
                depformer_input = self.user_depformer_in[cb_idx](transformer_out)
            else:
                depformer_input = self.user_depformer_in[0](transformer_out)

            # 前のトークンの埋め込み
            if cb_idx == 0:
                # テキストトークンを条件とする
                last_token_emb = self.user_depformer_text_emb(sequence[:, 0, :])
            else:
                # 前のユーザーコードブックを条件とする
                last_token_emb = self.user_depformer_emb[cb_idx - 1](
                    user_codes[:, cb_idx - 1, :]
                )

            depformer_input = depformer_input + last_token_emb

            # Depformer適用
            dep_output = self.user_depformer(depformer_input)

            # 正規化と線形層
            logits = self.user_linears[cb_idx](
                self.user_depformer_norms[cb_idx](dep_output)
            )

            all_logits.append(logits)

        # [B, user_dep_q, T, card]
        return torch.stack(all_logits, dim=1)

    def forward(
        self,
        codes: torch.Tensor,
        condition_tensors: tp.Optional[ConditionTensors] = None,
        predict_user: bool = False,  # 新規フラグ
    ) -> LMOutput:
        """
        訓練時のフォワードパス

        Args:
            codes: [B, K_total, T] - 全コードブック
            condition_tensors: 条件テンソル
            predict_user: ユーザー予測を有効化するか

        Returns:
            LMOutput (拡張版):
                - logits: システムオーディオのロジット
                - mask: システムオーディオのマスク
                - text_logits: テキストのロジット
                - text_mask: テキストのマスク
                - user_logits: ユーザーオーディオのロジット (新規)
                - user_mask: ユーザーオーディオのマスク (新規)
        """
        B, K, T = codes.shape

        # 既存の処理（遅延適用、初期トークン挿入）
        initial = self._get_initial_token().expand(B, -1, -1)
        delayed_codes = _delay_sequence(self.delays, codes, initial)
        delayed_codes = torch.cat([initial, delayed_codes], dim=2)

        # 条件付け
        sum_condition, cross_attention_src = None, None
        if condition_tensors is not None and self.fuser is not None:
            sum_condition = self.fuser.get_sum(condition_tensors)
            cross_attention_src = self.fuser.get_cross(condition_tensors)

        # メイントランスフォーマー + テキスト予測
        transformer_out, text_logits = self.forward_text(
            delayed_codes[:, :, :-1],
            sum_condition,
            cross_attention_src
        )

        # システムオーディオ予測（既存）
        system_logits = self.forward_depformer_training(
            delayed_codes[:, :, 1:],
            transformer_out
        )

        # ユーザーオーディオ予測（新規）
        user_logits = None
        user_mask = None
        if predict_user and self.predict_user_audio:
            user_logits = self.forward_user_depformer_training(
                delayed_codes[:, :, 1:],
                transformer_out
            )

            # マスクの作成（遅延を考慮）
            user_offset = self.user_audio_offset
            user_delays = self.delays[user_offset:user_offset + self.user_dep_q]
            user_logits, user_mask = _undelay_sequence(
                user_delays,
                user_logits,
                fill_value=float('NaN')
            )
            # ゼロトークンはマスク
            user_codes = codes[:, user_offset:user_offset + self.user_dep_q]
            user_mask &= (user_codes != self.zero_token_id)

        # システムのマスク処理（既存）
        system_logits, system_mask = _undelay_sequence(
            self.delays[self.audio_offset:self.audio_offset + self.dep_q],
            system_logits,
            fill_value=float('NaN')
        )
        system_mask &= (codes[:, self.audio_offset:self.audio_offset + self.dep_q] != self.zero_token_id)

        text_logits, text_mask = _undelay_sequence(
            self.delays[:1],
            text_logits,
            fill_value=float('NaN')
        )
        text_mask &= (codes[:, :1] != self.zero_token_id)

        # 拡張されたLMOutputを返す
        return LMOutputWithUserPrediction(
            logits=system_logits,
            mask=system_mask,
            text_logits=text_logits,
            text_mask=text_mask,
            user_logits=user_logits,
            user_mask=user_mask,
        )
```

#### 2.1.3 拡張されたLMOutput

```python
# moshi/models/lm.py

@dataclass
class LMOutputWithUserPrediction(LMOutput):
    """ユーザー予測を含む拡張LMOutput"""
    user_logits: torch.Tensor | None = None  # [B, K_user, T, card]
    user_mask: torch.Tensor | None = None    # [B, K_user, T]
```

### 2.2 LMGenクラスの拡張（推論時）

#### 2.2.1 推論状態の拡張

```python
# moshi/models/lm.py

@dataclass
class _LMGenStateWithUserPrediction(_LMGenState):
    """ユーザー予測を含む推論状態"""
    # 既存のフィールド
    cache: torch.Tensor
    initial: torch.Tensor
    graphed_main: CUDAGraphed
    graphed_depth: CUDAGraphed | None  # システム用Depformer
    offsets: torch.Tensor
    offset_cpu: int = 0

    # 新規フィールド
    graphed_user_depth: CUDAGraphed | None = None  # ユーザー用Depformer
    user_predictions_cache: torch.Tensor | None = None  # 予測されたユーザートークン
    user_prediction_confidence: torch.Tensor | None = None  # 予測の信頼度
```

#### 2.2.2 推論ステップの拡張

```python
class LMGen:
    def __init__(
        self,
        lm_model: LMModel,
        *args,
        enable_user_prediction: bool = False,  # ユーザー予測を有効化
        user_prediction_mode: str = "parallel",  # "parallel" or "sequential"
        user_prediction_confidence_threshold: float = 0.5,  # 信頼度閾値
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.lm_model = lm_model
        self.enable_user_prediction = enable_user_prediction
        self.user_prediction_mode = user_prediction_mode
        self.user_prediction_confidence_threshold = user_prediction_confidence_threshold

        # ユーザー予測が有効な場合のみ初期化
        if enable_user_prediction and hasattr(lm_model, 'user_depformer'):
            self.user_prediction_enabled = True
        else:
            self.user_prediction_enabled = False

    def _init_streaming_state_with_user_prediction(
        self,
        batch_size: int
    ) -> _LMGenStateWithUserPrediction:
        """ユーザー予測を含むストリーミング状態の初期化"""

        # 既存の状態初期化
        base_state = super()._init_streaming_state(batch_size)

        # ユーザー予測用の追加初期化
        if self.user_prediction_enabled:
            # ユーザーDepformer用のCUDAグラフ
            graphed_user_depth = self._build_user_depformer_graph(batch_size)

            # ユーザー予測キャッシュ
            user_predictions_cache = torch.zeros(
                batch_size,
                self.lm_model.user_dep_q,
                self.max_delay + 2,  # 循環バッファ
                device=self.device,
                dtype=torch.long,
            )

            # 信頼度スコア
            user_prediction_confidence = torch.zeros(
                batch_size,
                self.lm_model.user_dep_q,
                device=self.device,
                dtype=torch.float,
            )

            return _LMGenStateWithUserPrediction(
                **base_state.__dict__,
                graphed_user_depth=graphed_user_depth,
                user_predictions_cache=user_predictions_cache,
                user_prediction_confidence=user_prediction_confidence,
            )
        else:
            return base_state

    def _build_user_depformer_graph(self, batch_size: int) -> CUDAGraphed:
        """ユーザーDepformer用のCUDAグラフを構築"""

        def user_depformer_fn(text_token, transformer_out):
            """ユーザーコードブックを順次生成"""
            B = text_token.shape[0]
            device = text_token.device

            generated_tokens = []
            sequence = text_token[:, None, None]  # [B, 1, 1]

            for cb_idx in range(self.lm_model.user_dep_q):
                # ユーザーDepformerで予測
                logits = self.lm_model.forward_user_depformer(
                    cb_idx,
                    sequence,
                    transformer_out,
                    text_token if cb_idx == 0 else None,
                )

                # サンプリング
                token = sample_token(
                    logits.float(),
                    self.use_sampling,
                    self.temp,  # ユーザー用の温度
                    self.top_k,
                )

                generated_tokens.append(token[:, 0, 0])

                # 次のコードブック用に更新
                if cb_idx < self.lm_model.user_dep_q - 1:
                    sequence = token

            # [B, user_dep_q]
            return torch.stack(generated_tokens, dim=1)

        return CUDAGraphed(
            user_depformer_fn,
            batch_size=batch_size,
            device=self.device,
        )

    @torch.no_grad()
    def _step_with_user_prediction(
        self,
        input_tokens: torch.Tensor,
        use_predicted_user: bool = False,  # 予測されたユーザートークンを使用するか
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor] | None:
        """
        ユーザー予測を含む推論ステップ

        Args:
            input_tokens: 現在のユーザーオーディオコード [B, K_user, 1]
            use_predicted_user: Trueの場合、input_tokensの代わりに予測を使用

        Returns:
            (output_tokens, transformer_out, predicted_user_tokens) or None
            - output_tokens: [B, K_out, 1] システムの出力
            - transformer_out: [B, 1, dim] トランスフォーマー出力
            - predicted_user_tokens: [B, user_dep_q, 1] 予測されたユーザートークン
        """
        state = self._streaming_state
        if state is None or not isinstance(state, _LMGenStateWithUserPrediction):
            raise RuntimeError("Streaming state not properly initialized")

        lm_model = self.lm_model
        B, Ki, S = input_tokens.shape

        # ユーザートークンの処理
        actual_user_tokens = input_tokens

        if use_predicted_user and state.user_predictions_cache is not None:
            # 前のステップで予測したユーザートークンを使用
            # （投機的実行の場合）
            read_position = (state.offsets[:, None, None]) % state.user_predictions_cache.shape[2]
            predicted_user = state.user_predictions_cache.gather(dim=2, index=read_position)

            # 信頼度チェック
            confidence = state.user_prediction_confidence
            high_confidence_mask = (confidence > self.user_prediction_confidence_threshold)

            # 信頼度が高い場合は予測を使用、そうでない場合は実際の入力を使用
            actual_user_tokens = torch.where(
                high_confidence_mask[:, :, None],
                predicted_user,
                input_tokens
            )

        # 既存の_stepロジック（システム予測）
        # キャッシュに書き込み
        CT = state.cache.shape[2]
        delays = self.delays_cuda[lm_model.dep_q + 1:]
        write_positions = (state.offsets[:, None, None] + delays[:, None]) % CT
        scatter_with_mask_(
            state.cache[:, lm_model.dep_q + 1:],
            -1,
            write_positions,
            actual_user_tokens,
            state.exec_mask[:, None, None]
        )

        # 入力準備
        is_init = state.offsets[:, None, None] <= self.delays_cuda[:, None]
        is_init |= ~state.exec_mask[:, None, None]
        positions = (state.offsets % CT)[:, None, None].expand_as(is_init)
        input_ = state.cache.gather(dim=2, index=positions)
        input_ = torch.where(is_init, state.initial, input_)

        # CFG処理（省略）

        # メイントランスフォーマー + テキスト予測
        transformer_out, text_logits = state.graphed_main(input_, state.condition_sum, state.condition_cross)

        # テキストサンプリング
        text_token = sample_token(
            text_logits.float(),
            self.use_sampling,
            self.temp_text,
            self.top_k_text,
        )
        text_token = text_token[:, 0, 0]  # [B]

        # システムオーディオ生成（既存のDepformer）
        if state.graphed_depth is not None:
            system_audio_tokens = state.graphed_depth(text_token, transformer_out)
        else:
            system_audio_tokens = None

        # ユーザーオーディオ予測（新しいDepformer）
        predicted_user_tokens = None
        if self.user_prediction_enabled and state.graphed_user_depth is not None:
            predicted_user_tokens = state.graphed_user_depth(text_token, transformer_out)

            # 予測の信頼度を計算（エントロピーベース）
            # 実際には、Depformerからロジットも返す必要がある
            # ここでは簡略化

            # 予測をキャッシュに保存
            future_positions = (
                (state.offsets[:, None, None] + self.lm_model.user_prediction_lookahead)
                % state.user_predictions_cache.shape[2]
            )
            state.user_predictions_cache.scatter_(
                2,
                future_positions.expand(-1, self.lm_model.user_dep_q, -1),
                predicted_user_tokens[:, :, None]
            )

        # オフセット更新
        state.offsets = torch.where(state.exec_mask, state.offsets + 1, state.offsets)
        state.offset_cpu += 1

        # キャッシュに書き込み
        positions = (state.offsets % CT)[:, None, None]
        scatter_with_mask_(
            state.cache[:, :1],
            -1,
            positions,
            text_token[:, None, None],
            state.exec_mask[:, None, None]
        )

        if system_audio_tokens is not None:
            audio_tokens = system_audio_tokens[:, :, None]
            scatter_with_mask_(
                state.cache[:, 1 : lm_model.dep_q + 1, :],
                -1,
                positions.expand_as(audio_tokens),
                audio_tokens,
                state.exec_mask[:, None, None],
            )

        # 出力準備
        if not self.support_out_of_sync and state.offset_cpu <= self.max_delay:
            return None

        gen_delays_cuda = self.delays_cuda[: lm_model.dep_q + 1]
        index = (state.offsets[:, None, None] - self.max_delay + gen_delays_cuda[:, None]) % CT
        out = state.cache.gather(dim=2, index=index)

        mask = (state.offsets <= self.max_delay) | ~state.exec_mask
        out[mask, :, :] = lm_model.ungenerated_token_id

        return out, transformer_out, predicted_user_tokens

    @torch.no_grad()
    def step_with_user_prediction(
        self,
        input_tokens: torch.Tensor,
        use_predicted_user: bool = False,
    ) -> tuple[torch.Tensor | None, torch.Tensor | None]:
        """
        外部API: ユーザー予測を含む推論ステップ

        Returns:
            (system_output, user_prediction)
            - system_output: [B, K_out, 1] or None
            - user_prediction: [B, user_dep_q, 1] or None
        """
        result = self._step_with_user_prediction(input_tokens, use_predicted_user)

        if result is None:
            return None, None

        output_tokens, _, predicted_user_tokens = result
        return output_tokens, predicted_user_tokens
```

## 3. 訓練方法

### 3.1 訓練ループの拡張

```python
# kyutai-labs/moshi-finetune リポジトリで実装

def compute_loss_with_user_prediction(
    model: LMModel,
    batch: dict,
    user_prediction_weight: float = 0.1,
) -> tuple[torch.Tensor, dict]:
    """
    ユーザー予測を含む損失計算

    Args:
        model: LMModel
        batch: {
            'codes': [B, K_total, T],
            'condition_tensors': {...},
        }
        user_prediction_weight: ユーザー予測損失の重み

    Returns:
        (total_loss, metrics)
    """
    codes = batch['codes']
    condition_tensors = batch.get('condition_tensors')

    # フォワードパス（ユーザー予測を有効化）
    output = model.forward(
        codes,
        condition_tensors=condition_tensors,
        predict_user=True,  # ユーザー予測を有効化
    )

    # システム発話の損失（既存）
    system_loss = F.cross_entropy(
        output.logits.reshape(-1, model.card),
        codes[:, model.audio_offset:model.audio_offset + model.dep_q].reshape(-1),
        reduction='none',
    )
    system_loss = (system_loss * output.mask.reshape(-1)).sum() / output.mask.sum()

    # テキストの損失（既存）
    text_loss = F.cross_entropy(
        output.text_logits.reshape(-1, model.text_card),
        codes[:, :1].reshape(-1),
        reduction='none',
    )
    text_loss = (text_loss * output.text_mask.reshape(-1)).sum() / output.text_mask.sum()

    # ユーザー発話の損失（新規）
    user_loss = torch.tensor(0.0, device=codes.device)
    if output.user_logits is not None and output.user_mask is not None:
        user_offset = model.user_audio_offset
        user_targets = codes[:, user_offset:user_offset + model.user_dep_q]

        user_loss = F.cross_entropy(
            output.user_logits.reshape(-1, model.card),
            user_targets.reshape(-1),
            reduction='none',
        )
        user_loss = (user_loss * output.user_mask.reshape(-1)).sum() / output.user_mask.sum()

    # 合計損失
    total_loss = system_loss + text_loss + user_prediction_weight * user_loss

    metrics = {
        'loss/system': system_loss.item(),
        'loss/text': text_loss.item(),
        'loss/user': user_loss.item(),
        'loss/total': total_loss.item(),
    }

    return total_loss, metrics
```

### 3.2 訓練設定

```python
# 訓練設定例

training_config = {
    # モデル設定
    'model': {
        'predict_user_audio': True,          # ユーザー予測を有効化
        'user_n_q': 8,                       # ユーザーコードブック数
        'user_dep_q': 8,                     # ユーザーDepformerで生成する数
        'user_prediction_lookahead': 1,      # 1ステップ先を予測
        'user_depformer_dim': 256,
        'user_depformer_multi_linear': True,
    },

    # 損失重み
    'loss_weights': {
        'system_audio': 1.0,
        'text': 1.0,
        'user_audio': 0.1,  # ユーザー予測は補助タスク
    },

    # 段階的訓練
    'curriculum': {
        'stage1': {
            'epochs': 10,
            'user_prediction_weight': 0.0,  # まずシステム予測のみ
        },
        'stage2': {
            'epochs': 10,
            'user_prediction_weight': 0.05,  # 徐々にユーザー予測を追加
        },
        'stage3': {
            'epochs': 20,
            'user_prediction_weight': 0.1,  # フル重み
        },
    },
}
```

## 4. 推論時の使用方法

### 4.1 基本的な使用法

```python
# 推論時の例

# モデル初期化
lm_model = LMModel(
    predict_user_audio=True,
    user_dep_q=8,
    # ... その他のパラメータ
)

# LMGen初期化
lm_gen = LMGen(
    lm_model,
    enable_user_prediction=True,
    user_prediction_mode="parallel",
    user_prediction_confidence_threshold=0.5,
)

# ストリーミング開始
with lm_gen.streaming(batch_size=1):
    for user_audio_frame in audio_stream:
        # ユーザー音声をエンコード
        user_codes = mimi.encode(user_audio_frame)

        # 推論（システム出力 + ユーザー予測）
        system_output, user_prediction = lm_gen.step_with_user_prediction(
            user_codes,
            use_predicted_user=False,  # まずは実際の入力を使用
        )

        if system_output is not None:
            # システム音声をデコード
            system_audio = mimi.decode(system_output[:, 1:, :])

            # ユーザー予測を確認（デバッグ用）
            if user_prediction is not None:
                print(f"Predicted user tokens: {user_prediction}")
```

### 4.2 投機的実行モード

```python
# 投機的実行: 予測を使って先読み推論

with lm_gen.streaming(batch_size=1):
    predicted_user_buffer = deque(maxlen=5)

    for t, user_audio_frame in enumerate(audio_stream):
        user_codes = mimi.encode(user_audio_frame)

        # ユーザー予測を使用して推論
        # (前のステップで予測したトークンを実際の入力として使用)
        system_output, user_prediction = lm_gen.step_with_user_prediction(
            user_codes,
            use_predicted_user=(t > 0),  # 最初のステップ以降は予測を使用
        )

        if user_prediction is not None:
            predicted_user_buffer.append(user_prediction)

        if system_output is not None:
            # 実際のユーザー入力と予測を比較
            if len(predicted_user_buffer) > 0:
                accuracy = compute_prediction_accuracy(
                    predicted_user_buffer[0],
                    user_codes
                )
                print(f"User prediction accuracy: {accuracy:.2%}")

            system_audio = mimi.decode(system_output[:, 1:, :])
```

## 5. 利点と課題

### 5.1 利点

1. **明示的な予測**:
   - ユーザー発話を明示的にモデル化
   - 訓練時に明確な学習シグナル

2. **柔軟性**:
   - 予測を使用するか、実際の入力を使用するかを選択可能
   - 信頼度に基づく適応的な動作

3. **投機的実行**:
   - 予測を使って先読み推論
   - レイテンシの低減の可能性

4. **既存アーキテクチャとの親和性**:
   - システムDepformerと同じ構造
   - 既存のストリーミング機構を活用

### 5.2 課題

1. **計算コスト**:
   - 追加のDepformerで計算量が増加
   - 推論速度への影響

   **解決策**:
   - CUDAグラフで最適化
   - 必要に応じてユーザーDepformerを小型化

2. **予測精度**:
   - ユーザー発話は予測困難
   - 誤予測時のフォールバック

   **解決策**:
   - 信頼度スコアで選択的に使用
   - 短期予測に限定（1-2ステップ）

3. **訓練の複雑さ**:
   - マルチタスク学習のバランス調整
   - 段階的訓練が必要

   **解決策**:
   - カリキュラム学習
   - 適切な損失重み設定

4. **メモリ使用量**:
   - 追加のパラメータと状態
   - バッチサイズへの影響

   **解決策**:
   - パラメータ共有の検討
   - 効率的なキャッシュ管理

## 6. 評価方法

### 6.1 予測精度の評価

```python
def evaluate_user_prediction_accuracy(
    model: LMModel,
    lm_gen: LMGen,
    test_data: DataLoader,
) -> dict:
    """ユーザー予測精度を評価"""

    metrics = {
        'token_accuracy': [],
        'sequence_accuracy': [],
        'confidence_correlation': [],
    }

    with lm_gen.streaming(batch_size=1):
        for batch in test_data:
            user_codes_gt = batch['user_codes']  # [B, K, T]

            for t in range(user_codes_gt.shape[2] - 1):
                current_codes = user_codes_gt[:, :, t:t+1]
                next_codes_gt = user_codes_gt[:, :, t+1:t+2]

                # 予測
                _, user_pred = lm_gen.step_with_user_prediction(current_codes)

                if user_pred is not None:
                    # トークンレベル精度
                    token_acc = (user_pred == next_codes_gt).float().mean()
                    metrics['token_accuracy'].append(token_acc.item())

                    # シーケンスレベル精度（全コードブックが一致）
                    seq_acc = (user_pred == next_codes_gt).all(dim=1).float().mean()
                    metrics['sequence_accuracy'].append(seq_acc.item())

    return {
        'token_accuracy': np.mean(metrics['token_accuracy']),
        'sequence_accuracy': np.mean(metrics['sequence_accuracy']),
    }
```

### 6.2 対話品質への影響

```python
def evaluate_dialogue_quality_with_user_prediction(
    baseline_model: LMGen,
    user_pred_model: LMGen,
    dialogue_data: list,
) -> dict:
    """ユーザー予測が対話品質に与える影響を評価"""

    metrics = {
        'response_appropriateness': [],
        'turn_taking_naturalness': [],
        'interruption_handling': [],
    }

    for dialogue in dialogue_data:
        # ベースライン（ユーザー予測なし）
        baseline_response = generate_dialogue(baseline_model, dialogue)

        # ユーザー予測あり
        user_pred_response = generate_dialogue(user_pred_model, dialogue)

        # 人間評価またはメトリクスベース評価
        metrics['response_appropriateness'].append(
            compute_appropriateness_score(user_pred_response, dialogue)
        )

        # ターンテイキングの自然さ
        metrics['turn_taking_naturalness'].append(
            compute_turn_taking_score(user_pred_response)
        )

    return metrics
```

## 7. まとめ

アプローチ1「専用のDepformer分岐で完全予測」は、最も包括的なユーザー発話予測手法です。

**主な特徴**:
- システムDepformerと同様の構造でユーザー発話を予測
- 明示的な訓練シグナルで学習
- 投機的実行による先読み推論が可能

**推奨される使用ケース**:
- 研究目的でユーザー予測の限界を探る
- 十分な計算リソースがある場合
- 高品質な対話データが利用可能な場合

**次のステップ**:
1. プロトタイプ実装
2. 小規模データでの概念実証
3. 予測精度と計算コストのトレードオフ評価
4. 本格的な訓練とファインチューニング
